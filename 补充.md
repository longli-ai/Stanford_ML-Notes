


##### 在线课程内容

- 为什么叫回归
  - 回归的是一个概率p，s函数压缩到0-1
- 分类问题
  - 样本空间中，找到一个决策边界
  - 线性回归的值，采用压缩函数以后，判定其是正负样本

- 损失函数
  - 线性回归
    - MSE是凸函数
  - 逻辑回归
    - mse非凸函数
    - log loss（为什么是凸函数，可以不想）
    - 函数图像
- 多分类
  - one vs one
  	- 只区分第一类和第二类，无视第三类
  - one vs rest
  	- 区分第一类，和非第一类
  	- 个人觉得这个分法比较make sense

- 过拟合/high variance
  - 相当于把训练集背下来了
  - 训练集效果好，验证集学习并不好
  - 例子：非洲酋长短波收音机学习英语
- 欠拟合/high bias
  - 训练集和验证集的效果都不好，模型就不适合，需要提高模型能力


学习率
- 超参数
	- 不收敛，第一时间检查学习率
	- 收敛步长，太大或者太小，均有问
	- 找到一个差不多的学习率就行了


为什么需要解决分类问题
- 线性回归健壮性不够，噪声影响很大

算法应用经验
LR和其他模型对比
- LR可以作为baseline模型
- lr能以概率的形式输出结果，非0，1判定
- lr的可解释性性强，可控度高
- lr训练筷，feature engineering之后效果好
- 因为结果是概率，可以做ranking model
- 添加feature简单



















## 决策树与随机森林

具有很好的数据预处理特性

- 从lr到决策树
    - 总体流程与核心问题
        - 简单（规则），逻辑清晰（条件判断），可解析性强（知道为什么）
        - 透明的模型
        
        - 内部节点对于属性进行判断
        - 每个分支是测试的可能结果（某个取值）
        - 叶子结点是预测结果

        - 总体流程（怎么长）
            - 分而治之(divide and conquer)
                - 自根至叶的递归过程
                - 每个中间节点寻找一个划分（split or test) 属性
        - 三种停止条件（什么时候停）
            - 看截图，做成anki
            - 1一个属性划分
            - 2属性用完了
            - 3有属性，但没样本
    - 熵，信息增益，信息增益率
        - 熵： 衡量样本集合纯度的标准
            - 决策树希望分叉纯度越高越好，不确定性越低越好
        - 信息增益：（id3)
            - 数量比例去做加权
            - 基于西瓜做思考
            - 缺点
                - 按照“编号”进行划分，每个样本一个节点
                - 新样本如何划分？
        - 信息增益率（c4.5)
        - 基尼系数(cart)
            - cart是二叉树
        - 三种不同的决策树
            - id3
                - 取值多的属性，更容易使数据更纯，其信息增益更大
            - c4.5
                - 信息增益率替代信息增益
            - cart
                - 最小化不纯度
- 回归树
    - 回归数背后的含义
        - 垂直于属性坐标值
    - 构建回归书
        - 启发式学习
        - 自顶向下的贪婪式递归方案
            - sss
            - 贪婪：只考虑当前
        - rss结果最小
            - 切分维度以及切分点rss结果最小
            - 动态规划dp
    - 最优化回归书
        - 回归树剪枝
            - 如果让回归树充分生长，同样会有过拟合
            - 正则化 alpha叶子结点的数字

- 从决策树到随机森林
    - 采样与bootstrap
        - 样本进行bootstrap取样，有放回的取样
    - bagging思想（boostrap aggregating缩写）
        - 为什么，降低过拟合风险，提高泛化能力
        - 分类问题做投票
        - 回归问题求平均
            - 不会容易收到noise的影响
    - bagging与随机森林
        - 随机森林
            - 特征也进行bootstrap取样

具有很好的预处理特性

决策树也能完成回归问题

xgboost









下面是面试题目





##### 










##### 在线课程
SVM模型
- 函数间隔与几何间隔
    - 无数条线，那个线最好











##### 吴恩达 机器学习

- C = 1/$\lambda​$
    - $\lambda$ 是正则化项的超参数
    - $\lambda$太小，C很大，决策边界很敏感，会因为异常点change
    - $\lambda$太大，C很小，决策边界不敏感，可能分类不是很好



- 为什么svm决策边界

核函数



##### 偏差bias

- underfitting
    - $J_{train}$ will be low
    - $J_{CV} \gg J_{train}$
- 吴大师的解释
    - the learning algorithm have a very strong perconception/bias
    - 不顾数据事实，仍用现在model来拟合

##### 方差variance

- overfitting
    - $J_{train}​$ will be high
    - $J_{CV} \approx J_{train}​$
- 一个随机变量的**方差**描述的是它的离散程度
- $Var(X) = E[(X-\mu)^2]$
- 吴大师的解释
    - 假设函数能拟合所有的数据，就是函数太过庞大，变量太多。
    - 没有足够的数据来约束假设函数





##### debugging a learning algorithm

- step 1, high variance or high bias





#####  SVM kernel

- linear kernel(no kernel) 
    - 当你样本特征很多，但是样本的数量很少
        - 采用线性核或者逻辑回归，因为没有足够多的数据拟合多项式函数
    - 样本特征少，样本的数量很多
        - 创造更多的特征，采用线性内核或者逻辑回归
- guassia kernel
    - 样本特征少，样本中等，可以采用高
- 逻辑回归
    - 和linear kernel svm是相似的，输入和输出都像，为什么相似？
- 神经网络
    - 适用于上面所有情况，缺点是速度慢




神经网路
- 梯度弥散和梯度爆炸含义和预防
















####  吴大师无监督学习

- K-means

- K-means有什么问题？如何解决这些问题？
    -  差的局部最优 local optima
        - 尝试多次随机初始化
        - 计算歧化损失函数，选择一个小的函数
            - k=2-10,比较好的
            - k比较大，随机初始化，不会得到太好的分类
    - 怎么去选择K
        - elbow method, 肘部方法
            - 计算损失函数
        - 如果看不出肘部点
            - 根据后续下游目标进行选择K
                - 比如t桖市场分割，分成3或者5类





##### 吴大师课程

- 降维
    - 数据压缩
        - 特征高度相关，可以将两个特征压缩成一个新特征，2D to 1D
    - PCA
        - 试图找到一个低维平面或者线，对数据进行投影，以便最小化**投影**误差的平方，最小化每个点与投影后的点距离的平方
    - pca于线性模型的区别
        - pca是最小化投影的距离(点垂直于pca的平面)，x1，x2是平等的，没有predict只说
        - 线性模型是最小化predict和y之间最小化
    - PCA流程
        - 数据预处理：平均归一化/feature scaling
    - pca提高监督学习速度
    - 使用pca的误区
        - 不要用pca去预防overfitting
            - pca降维后，特征更少了，不利于拟合，还是老实去用正则化
        - 设置ml系统，直接使用pca
            - 不建议使用pca，建议使用原始数据后，如果用了原始没用，才能建议去用pca



##### 高斯分布/正态分布

- 平均值中心

    - $\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$

- 方差variance

    - $\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)^2$






