## Week4


### 神经网络表征



#### 动机
非线性假设
- 两个特征，工作很好
- 特征变多，二次项特征组合变多了，计算变的繁重

二次项特征组合$x_i * x_j$个数计算
- 比如$2,500$个灰度像素点，计算一下$x_i * x_j$的个数
- $2,500*2,500 /2 = 3,125,000$
- 除以2的原因是$x_i * x_j$和$x_j * x_i$是相同的


神经网络背景
- 模仿人脑神经元的工作原理，树突(输入)，轴突(输出）
- 大脑的学习方式有点像迁移学习
    - 比如有个区域是负责听的，切断听的途径，他会自己连接比如眼，并且通过眼得到和听一样的识别

#### 神经网络

模型表征
- 模仿人脑神经元的工作原理，树突(输入)，轴突(输出）
- 神经元模型：逻辑单元

人脑神经元结构
- 树突-输入，轴突-输出，细胞体-激活函数单位

![](https://user-images.githubusercontent.com/41643043/56087332-c041f280-5e9b-11e9-85c5-502245ba329a.png)

神经元模型
![](https://user-images.githubusercontent.com/41643043/56087333-c20bb600-5e9b-11e9-8d14-65c32fd28ff1.png)

神经网络
- activation定义
    - 上标$j$是当前层数
- weights的定义
    - 上标$j$到$j+1$的参数
    - 下标$kl$表示第$k$个神经元的第$l$个参数
- weights的矩阵大小定义
    - 矩阵大小$s_{j+1} * (s_j+1)$

![](https://user-images.githubusercontent.com/41643043/56087410-66422c80-5e9d-11e9-9575-f7c992645a81.png)


**前向传播** forward propagation
- sigmoid压缩值z的定义
    - 上标和activation一样，但是z代表输入前，a代表输出
    - 即使通过激励g，但a和z的维度相同
- 从输入层依次计算，将激励传递给下层神经元

![](https://user-images.githubusercontent.com/41643043/55844334-65dd2500-5b6f-11e9-8ae3-af515d06b454.png)


#### 应用

为什么神经网络能学习复杂的非线性假设
- 每一层的神经元，都能在上层的基础上进行更加复杂的非线性计算


XNOR实例实现
- AND实现
    - $y = g(-30 + 20x_1+20x_2);\ x_1,x_2\in \{0,1\}$ 
- OR实现
    - $y = g(-10 + 20x_1+20x_2);\ x_1,x_2\in \{0,1\}$ 
- NOT实现
    - $y = g(-10 + 20x_1);\ x_1\in \{0,1\}$ 
- XNOR实现: $(NOT\ x_1)\ AND\ (NOT\ x_2)$
    - $y = g(10 - 20x_1 - 20x_2);\ x_1,x_2\in \{0,1\}$ 
- XOR实例实现: $ (x_1\ AND\ x_2)\ OR\ (x_1\ XNOR\ x_2)$


多分类
- 多分类输出单元：one-vs-all
- 前面都是单个神经元输出，多分类是向量化输出

![](https://user-images.githubusercontent.com/41643043/56087834-c341e080-5ea5-11e9-9146-77e464b2d372.png)

