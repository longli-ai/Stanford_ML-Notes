## Week7

### Support Vector Machines


优化目标
- 从逻辑回归引入代价函数，将代价函数换成hinge-lost
- 开始优化hinge-lost，求最小化

假设函数
$$h_\theta(x) = \left.\{\begin{array}{cc} 
		1, & \theta^Tx\geq 0\\ 
		0, & other 
\end{array}\right. \tag{2}$$

大距离分类器 分类算法
- 保持一个安全距离

$min_\theta C\sum^{m}_{}$


为什么转变成优化正则化项
- 公式1$C$很大

鲁棒性很好


背后的原理
向量内积
- $u^Tv = p ||u|| = u_1v_1 + u_2v_2$

u的范数
- $||u|| = \sqrt{u_1^2+u_2^2}$
- u向量的长度
- p是v在u上的投影长度


### 核方法

高斯内核$k(x,l^{(i)})$
$$f_1 = similarity(x,l^{(1)}) = exp(-\frac{||x-l^{(1)}||^2}{2\sigma^2})$$




### 实践

线性内核（无内核）
- 不用内核，适用于特征维度n比较大，而样本m相对较少
- 不需要比较高级的内核，否则会到这过拟合

高斯内核
- 适用于特征维度n小，样本m比较大，这样能很好的拟合非线性


使用高斯内核之前，对样本的特征进行幅度缩放，否则进行范数操作时候，有些大数字特征会起主要作用


内核要符合定理


可能遇到的内核，了解即可
- 多项式内核 
    - $k(x,l)=(x^Tl+c)^d$


多分类
- 训练k分类个svm

模型选择
- 特征维度n很大，样本m较小，采用逻辑回归或者无内核的svm
- 特征维度n较小，样本m中等，采用高斯内核
- 特征维度n较小，样本m较大，采用逻辑回归或者无内核的svm
    - 高斯内核很慢

逻辑回归或者无内核的svm
- 两个模型很像
- svm适合非线性复杂的

debug-调试














