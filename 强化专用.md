





## 逻辑回归

##### 问题

介绍一下？

- 是个解决分类问题的模型

公式推导？

损失函数及其图像？

与线性回归的关系？

- 线性回归处理回归问题

你在什么地方用过？

怎么用的？



##### 在线课程内容

- 为什么叫回归
  - 回归的是一个概率p，s函数压缩到0-1
- 分类问题
  - 样本空间中，找到一个决策边界
  - 线性回归的值，采用压缩函数以后，判定其是正负样本

- 损失函数
  - 线性回归
    - MSE是凸函数
  - 逻辑回归
    - mse非凸函数
    - log loss（为什么是凸函数，可以不想）
    - 函数图像
- 多分类
  - one vs one
  	- 只区分第一类和第二类，无视第三类
  - one vs rest
  	- 区分第一类，和非第一类
  	- 个人觉得这个分法比较make sense

- 过拟合/high variance
  - 相当于把训练集背下来了
  - 训练集效果好，验证集学习并不好
  - 例子：非洲酋长短波收音机学习英语
- 欠拟合/high bias
  - 训练集和验证集的效果都不好，模型就不适合，需要提高模型能力


学习率
- 超参数
	- 不收敛，第一时间检查学习率
	- 收敛步长，太大或者太小，均有问
	- 找到一个差不多的学习率就行了


为什么需要解决分类问题
- 线性回归健壮性不够，噪声影响很大

算法应用经验
LR和其他模型对比
- LR可以作为baseline模型
- lr能以概率的形式输出结果，非0，1判定
- lr的可解释性性强，可控度高
- lr训练筷，feature engineering之后效果好
- 因为结果是概率，可以做ranking model
- 添加feature简单













##正则化

- 解释下正则化？
	- 控制参数的幅度，不让模型过拟合
	- 限制参数搜索空间
		- 控制高次相$\theta$的值不能太大
		- lambda也是超参数
- 给参数假设一个先验分布（prml书)
	- L1/L2正则化补充





## 决策树与随机森林

具有很好的数据预处理特性

- 从lr到决策树
    - 总体流程与核心问题
        - 简单（规则），逻辑清晰（条件判断），可解析性强（知道为什么）
        - 透明的模型
        
        - 内部节点对于属性进行判断
        - 每个分支是测试的可能结果（某个取值）
        - 叶子结点是预测结果

        - 总体流程（怎么长）
            - 分而治之(divide and conquer)
                - 自根至叶的递归过程
                - 每个中间节点寻找一个划分（split or test) 属性
        - 三种停止条件（什么时候停）
            - 看截图，做成anki
            - 1一个属性划分
            - 2属性用完了
            - 3有属性，但没样本
    - 熵，信息增益，信息增益率
        - 熵： 衡量样本集合纯度的标准
            - 决策树希望分叉纯度越高越好，不确定性越低越好
        - 信息增益：（id3)
            - 数量比例去做加权
            - 基于西瓜做思考
            - 缺点
                - 按照“编号”进行划分，每个样本一个节点
                - 新样本如何划分？
        - 信息增益率（c4.5)
        - 基尼系数(cart)
            - cart是二叉树
        - 三种不同的决策树
            - id3
                - 取值多的属性，更容易使数据更纯，其信息增益更大
            - c4.5
                - 信息增益率替代信息增益
            - cart
                - 最小化不纯度
- 回归树
    - 回归数背后的含义
        - 垂直于属性坐标值
    - 构建回归书
        - 启发式学习
        - 自顶向下的贪婪式递归方案
            - sss
            - 贪婪：只考虑当前
        - rss结果最小
            - 切分维度以及切分点rss结果最小
            - 动态规划dp
    - 最优化回归书
        - 回归树剪枝
            - 如果让回归树充分生长，同样会有过拟合
            - 正则化 alpha叶子结点的数字

- 从决策树到随机森林
    - 采样与bootstrap
        - 样本进行bootstrap取样，有放回的取样
    - bagging思想（boostrap aggregating缩写）
        - 为什么，降低过拟合风险，提高泛化能力
        - 分类问题做投票
        - 回归问题求平均
            - 不会容易收到noise的影响
    - bagging与随机森林
        - 随机森林
            - 特征也进行bootstrap取样

具有很好的预处理特性

决策树也能完成回归问题

xgboost









下面是面试题目

ID3？C4.5？CART？区别，优点、缺点，计算公式？
为什么DT不适合稀释数据？
预剪枝？后减枝？
3.1 Adaboost
介绍一下Adaboost？手推Adaboost
3.2 GBDT
GBDT腾讯面试必问，因为我去的鹅厂现场面试的，被问到了GBDT的相关内容，而旁边一个面试官在电话面试，也是在问GBDT的内容。
介绍下GBDT？GBDT的基模型是什么，是分类树还是回归树？Gradient的体现？为什么要使用多棵树来不断缩小残差？而不是使用一棵拟合得更好的数？GBDT如何构建特征？GBDT如何用于分类，写出公式？
3.3 XGBoost
手推XGBoost必备的。
你在什么地方用到了XGBoost？它和GBDT的区别在哪里？你为什么不用LightGBM？
XGBoost不手推一下，说的内容感觉会很苍白。
3.4 随机森林（dt高级模型）
解释下随机森林？
你用它做过什么？为什么可以用它？
解释下bagging？



##### 









## 支持向量机
手推SVM，必备的。
SVM的损失函数是什么，为什么选择它作为损失函数？
为什么用对偶？
核函数是什么，有什么作用？你用过哪几种核函数？这里最好写出来。

##### 在线课程
SVM模型
- 函数间隔与几何间隔
    - 无数条线，那个线最好











##### 吴恩达 机器学习

- C = 1/$\lambda​$
    - $\lambda$ 是正则化项的超参数
    - $\lambda$太小，C很大，决策边界很敏感，会因为异常点change
    - $\lambda$太大，C很小，决策边界不敏感，可能分类不是很好



- 为什么svm决策边界

核函数



##### 偏差bias

- underfitting
    - $J_{train}$ will be low
    - $J_{CV} \gg J_{train}$
- 吴大师的解释
    - the learning algorithm have a very strong perconception/bias
    - 不顾数据事实，仍用现在model来拟合

##### 方差variance

- overfitting
    - $J_{train}​$ will be high
    - $J_{CV} \approx J_{train}​$
- 一个随机变量的**方差**描述的是它的离散程度
- $Var(X) = E[(X-\mu)^2]$
- 吴大师的解释
    - 假设函数能拟合所有的数据，就是函数太过庞大，变量太多。
    - 没有足够的数据来约束假设函数





##### debugging a learning algorithm

- step 1, high variance or high bias





#####  SVM kernel

- linear kernel(no kernel) 
    - 当你样本特征很多，但是样本的数量很少
        - 采用线性核或者逻辑回归，因为没有足够多的数据拟合多项式函数
    - 样本特征少，样本的数量很多
        - 创造更多的特征，采用线性内核或者逻辑回归
- guassia kernel
    - 样本特征少，样本中等，可以采用高
- 逻辑回归
    - 和linear kernel svm是相似的，输入和输出都像，为什么相似？
- 神经网络
    - 适用于上面所有情况，缺点是速度慢















## 深度学习部分
解释一下交叉熵、相对熵，它们的关系？互信息呢？
Cross Entropy是什么，它和log loss的区别？
Dropout防止过拟合的原理？
CNN如何用于NLP？CNN和RNN在NLP中能到达什么效果，怎么选择用哪一个？
梯度消失？梯度爆炸？
解释下LSTM，画出结构？GRU？为什么用LSTM不用RNN？





## 特征工程
如何处理缺失值？异常值？为什么这么处理？
怎么降维？遇到的数据最大多少维？你怎么做？为什么这么做？
类别型数据的处理？连续性数据的处理？你怎么分桶的？
数据不平衡怎么做？要注意什么？





## 聚类算法
你用过什么聚类算法？解释下原理？
K-means有什么问题？如何解决这些问题？
说一说“中餐馆问题”——抱歉，这个我现在还没搞明白。





####  吴大师无监督学习

- K-means

- K-means有什么问题？如何解决这些问题？
    -  差的局部最优 local optima
        - 尝试多次随机初始化
        - 计算歧化损失函数，选择一个小的函数
            - k=2-10,比较好的
            - k比较大，随机初始化，不会得到太好的分类
    - 怎么去选择K
        - elbow method, 肘部方法
            - 计算损失函数
        - 如果看不出肘部点
            - 根据后续下游目标进行选择K
                - 比如t桖市场分割，分成3或者5类





##### 吴大师课程

- 降维
    - 数据压缩
        - 特征高度相关，可以将两个特征压缩成一个新特征，2D to 1D
    - PCA
        - 试图找到一个低维平面或者线，对数据进行投影，以便最小化**投影**误差的平方，最小化每个点与投影后的点距离的平方
    - pca于线性模型的区别
        - pca是最小化投影的距离(点垂直于pca的平面)，x1，x2是平等的，没有predict只说
        - 线性模型是最小化predict和y之间最小化
    - PCA流程
        - 数据预处理：平均归一化/feature scaling
    - pca提高监督学习速度
    - 使用pca的误区
        - 不要用pca去预防overfitting
            - pca降维后，特征更少了，不利于拟合，还是老实去用正则化
        - 设置ml系统，直接使用pca
            - 不建议使用pca，建议使用原始数据后，如果用了原始没用，才能建议去用pca



##### 高斯分布/正态分布

- 平均值中心

    - $\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$

- 方差variance

    - $\sigma^2 = \frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)^2$

    

##### 推荐系统

- 



## 降维
解释下PCA？LDA？SVD？区别？手推。
解释下协同过滤？



## 模型评估
Accuracy、Precision、Recall、F1-score、ROC、AUC
以上，概念、公式？
数据不平衡用什么评估指标？
P-R曲线和ROC的区别？







## NLP方向的问题：
1.TF-IDF
原理、公式？为什么用log？为什么“+1”？缺点？

2.NNLM、Word2vec、Doc2vec、FastText
介绍一下，解释一下模型？
怎么选择使用Word2vec、Doc2vec？
解释下Huffman Tree或Negative Sampling？详细说下Huffman Tree或Negative Sampling的过程？
Word2vec有什么优点、缺点？
Word2vec的模型参数怎么设置的？
余弦相似度？
为什么使用余弦相似度？

3.Seq2seq、attention机制、self-attention机制、Transformer、bert
介绍下模型？
attention机制、self-attention机制的计算过程？为什么使用attention？它们的优点？
bert的结构和应用？bert的为什么这么厉害？

CNN→RNN→LSTM/GRU→seq2seq→seq2seq-Attention→Transformer-self Attention→bert
这是一条线，我在说RNN的部分时，就把这条线顺着说完。

NNLM→Word2vec→Doc2vec→FastText→ELMO→GPT→Bert
这是另一条线，被问到Word2vec时我先退回去说NNLM，然后在Word2vec，要是面试官不反感，又继续沿着线走。



## 其他
1.根据简历上写的项目来进行具体的提问？
数据量？数据来源？为什么想到这么做？这么做优点、缺点？
Python迭代器和生成器？





2.算法题
不说了，我面了那么多，一道题都没答上来。校长说的四大金刚之一：leetcode（小编注：校长的口头禅四大金刚指的就是：课程 题库 OJ 竞赛），因为自身原因，我战略性放弃了，之后一定要补起来。
去某手机厂面试的时候，上来直接来3道题，把我干懵了；
对于这方面能力和我不相伯仲的朋友，我建议电话面试优先！一般电话面试不会去问算法题。







3.非技术面
为什么转行？
这是转行的朋友肯定会被问到的问题，一定想好说辞，好好准备。
未来规划？
薪资期望？
你遇到的最困难的事？你如何解决的？
评价下你自己？

你有什么问题要问我们？

- 你们的数据都从哪里来？
    - 买的，还是公司能搞到
- 你们需要自己打label么？
    - 有没有外包，需不需要自己打
- 你们工作时间是怎样的？
- 你们一线技术经理还有多少下项目的，比如给个比例？